{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232309\n",
      "ï»¿DOROTHY AND THE WIZARD IN OZ\n",
      "\n",
      "  BY\n",
      "\n",
      "  L. FRANK BAUM\n",
      "\n",
      "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
      "\n",
      "  ILLUSTRATED BY JOHN R. NEILL\n",
      "\n",
      "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW Y\n"
     ]
    }
   ],
   "source": [
    "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "          text=f.read()\n",
    "print(len(text))\n",
    "print(text[:200]) # getting the first 200 characters of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
      "length of chars list:  81\n"
     ]
    }
   ],
   "source": [
    "chars=sorted(set(text)) # get the sorted set of characters in the text\n",
    "print(chars)\n",
    "print('length of chars list: ', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 58, 65, 65, 68]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# takes in an input string\n",
    "# creates a mapping of characters to their integer index - i.e. {a:0, b:1, c:2}\n",
    "# returns a list of integers representing the encoding of the input string using the character map\n",
    "# character level tokenizer (small vocabulary with a lot of tokens to convert)\n",
    "\n",
    "# encoding strings to integer\n",
    "def encode(inputString):\n",
    "    string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "    return [string_to_int[c] for c in inputString]\n",
    "\n",
    "encodeHello=encode('hello')\n",
    "print(encodeHello)\n",
    "\n",
    "# decoding integer encoding to string\n",
    "def decode(encodedList):\n",
    "    int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "    return ''.join([int_to_string[i] for i in encodedList])\n",
    "\n",
    "decodeHello=decode(encodeHello)\n",
    "print(decodeHello)\n",
    "\n",
    "# this is an example of a CHARACTER LEVEL TOKENIZER where we have a small vocabulary (81 characters in this case) and a lot of tokens to encode\n",
    "# a WORD LEVEL TOKENIZER has a very large vocabulary but a small(er) amount of tokens to encode\n",
    "# a SUB LEVEL TOKENIZER is something in between a character level and word level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,  1, 47,\n",
      "        33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26, 49,  0,\n",
      "         0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,  0,  0,\n",
      "         1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1, 47, 33,\n",
      "        50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1, 36, 25,\n",
      "        38, 28,  1, 39, 30,  1, 39, 50,  9,  1])\n"
     ]
    }
   ],
   "source": [
    "# pytorch will help with a lot of the calculations/linear algebra\n",
    "# tensors: data structures representing multi-dimensional inputs (matrices, vectors, scalars)\n",
    "\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # turning the text into a super long sequence of integers (non-floating point numbers)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) target is tensor(28)\n",
      "when input is tensor([80, 28]) target is tensor(39)\n",
      "when input is tensor([80, 28, 39]) target is tensor(42)\n",
      "when input is tensor([80, 28, 39, 42]) target is tensor(39)\n",
      "when input is tensor([80, 28, 39, 42, 39]) target is tensor(44)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44]) target is tensor(32)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44, 32]) target is tensor(49)\n",
      "when input is tensor([80, 28, 39, 42, 39, 44, 32, 49]) target is tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# training and validation splits (80/20)\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# bigram language model: given a certain character, predict the next\n",
    "# hello\n",
    "# start -> h, h->e, e->l, l->l, l->o\n",
    "\n",
    "# block size: a hyperparameter determining the size of a data chunk used in batch training/processing\n",
    "# we're using it to make predictions and targets from the text\n",
    "# predictions: [5,67,21,58,40] 35  [:5]\n",
    "# targets: 5 [67,21,58,40,35]   [1:blockSize+1]\n",
    "# in the bigram language model we're seeing how far the prediction is from the target and will try to reduce that error\n",
    "\n",
    "block_size_test = 8\n",
    "\n",
    "x = train_data[:block_size_test] # [80, 28, 39, 42, 39, 44, 32, 49] 1\n",
    "y = train_data[1:block_size_test+1] # 80 [28, 39, 42, 39, 44, 32, 49,  1]\n",
    "\n",
    "for t in range(block_size_test):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print('when input is', context, 'target is', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "11.8\n",
      "False\n",
      "Device Count: 0\n"
     ]
    }
   ],
   "source": [
    "# GPU (graphics processing unit): designed to efficiently process large blocks of data simultaneously/in parallel\n",
    "# can do a simple task very efficiently and multiple at the same time \n",
    "# by putting blocks in a stack, we can give it to the GPU which can perform processing on the blocks scaled way up\n",
    "# GPUs differ from CPUs which can do sequential complex tasks\n",
    "\n",
    "# batch size: a hyperparameter telling how many blocks we are processing at the same time\n",
    "# batch size tells us the height of a block while block_size tells us the length of it\n",
    "\n",
    "# [1,2,3,4,5]\n",
    "# [1,2,3,4,5]\n",
    "# [1,2,3,4,5]\n",
    "# [1,2,3,4,5]\n",
    "\n",
    "# unable to get the GPU to work, so I'm stuck with the cpu (slower runtimes)\n",
    "# https://github.com/Infatoshi/fcc-intro-to-llms\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "print(device)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Device Count:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 61,  53, -33,  -5,  -4,  14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randint = torch.randint(-100, 100, (6,)) # returns a tensor with 6 values between -100 and 100\n",
    "randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 1.2000],\n",
       "        [2.2000, 3.1000],\n",
       "        [4.9000, 5.2000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.tensor([[0.1,1.2],[2.2,3.1],[4.9,5.2]]) # creating a 2x3 tensor with float values\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[-4.8935e+32,  7.5950e-43,  1.0000e+00],\n",
      "        [ 1.0000e+05,  1.0000e+10,  0.0000e+00]])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])\n",
      "tensor([1.0000e-10, 1.0000e-05, 1.0000e+00, 1.0000e+05, 1.0000e+10])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[      2331995282736,                   1,                   2],\n",
      "        [                  3,                   4, 4575657222473777152]])\n",
      "tensor([[4613937818241073152, 4617034042984890368, 4619004367821864960],\n",
      "        [4620833955170484224, 4621819117588971520, 4575657222473777152]])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch functions\n",
    "\n",
    "zeros = torch.zeros(2,3) # zeros(rows, columns) returns a tensor with just zeros\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(3,4)\n",
    "print(ones)\n",
    "\n",
    "empty = torch.empty(2,3)\n",
    "print(empty)\n",
    "\n",
    "arange = torch.arange(5)\n",
    "print(arange)\n",
    "\n",
    "linespace = torch.linspace(3,10,steps=5) # goes from 3 to 10 in 5 equal steps\n",
    "print(linespace)\n",
    "\n",
    "logspace = torch.logspace(start=-10,end=10,steps=5) # start at -10, then gets to 10 in 5 equal steps\n",
    "print(logspace)\n",
    "\n",
    "eye = torch.eye(5) # returns a tensor in reduced row echelon form with kxk dimensions if eye(k)\n",
    "print(eye)\n",
    "\n",
    "# torch.empty() and torch.empty_like() create tensors with uninitialized values - any values appearing in the tensors are random garbage values\n",
    "a = torch.empty((2,3),dtype=torch.int64)\n",
    "print(a)\n",
    "empty_like = torch.empty_like(a)\n",
    "print(empty_like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n",
      "tensor([1, 2, 3, 4, 5])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "torch.Size([4, 3, 2])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# linear algebra torch functions\n",
    "\n",
    "probs = torch.tensor([0.1,0.9]) # probabilities that add up to 1\n",
    "samples = torch.multinomial(probs, num_samples=10, replacement=True)\n",
    "print(samples)\n",
    "\n",
    "first = torch.tensor([1,2,3,4])\n",
    "out = torch.cat((first,torch.tensor([5])), dim=0) # concatenation of tensors\n",
    "print(out)\n",
    "\n",
    "# figuring out probabilities, then concatenating the predictions\n",
    "\n",
    "tril = torch.tril(torch.ones(5,5)) # tril = triangle lower - looks similar to how predictions build on each other\n",
    "print(tril)\n",
    "\n",
    "triu = torch.triu(torch.ones(5,5)) # triu = torch upper\n",
    "print(triu)\n",
    "\n",
    "# masked_fill? \n",
    "\n",
    "test = torch.zeros(2,3,4)\n",
    "test1 = test.transpose(0,2) # transposing - in this case, the values at elements 0 and 2 are swapped/transposed\n",
    "print(test1.shape)\n",
    "\n",
    "# stacking tensors together to make the giant stack to be passed to the GPU\n",
    "tensor1 = torch.tensor([1,2,3])\n",
    "tensor2 = torch.tensor([4,5,6])\n",
    "tensor3 = torch.tensor([7,8,9])\n",
    "\n",
    "stacked_tensor = torch.stack([tensor1,tensor2,tensor3])\n",
    "print(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8.9204, -0.5037, -1.5343], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "sample = torch.tensor([10.,10.,10.])\n",
    "linear = nn.Linear(3,3,bias=False) # nn.Linear\n",
    "print(linear(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "# exponentiating involving 2.71 = e\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tensor1a = torch.tensor([1.0,2.0,3.0])\n",
    "softmax_output = F.softmax(tensor1a,dim=0) # softmax converts a vector of numbers into a vector of probabilities\n",
    "\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100])\n"
     ]
    }
   ],
   "source": [
    "# embedding vectors\n",
    "\n",
    "# using nn.Embedding on a character level\n",
    "# an example is if our alphabet is 26 characters and we want vector embeddings of size 5 (a hyperparameter) for each character\n",
    "# the nn.Embedding layer can be visualized as a table with 26 rows and 5 columns\n",
    "\n",
    "# character --> embedding vector\n",
    "# 'a' --> [0.2,0.1,0.5,0.3,0.9] # these values are learned weights updated during back propagation\n",
    "# 'b' --> [0.7,0.8,0.2,0.6,0.4]\n",
    "# ...\n",
    "# 'z' --> [0.3,0.5,0.6,0.8,0.2]\n",
    "\n",
    "vocab_size=1000\n",
    "embedding_dim = 100 # how many dimensions will a vector embedding be?\n",
    "\n",
    "# creates an embedding layer (analagous to a lookup table) turning each token into a vector embedding of size embedding_dim\n",
    "# embedding layer contains matrix of learned weights with a shape of (vocab_size, embedding_dim)\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "\n",
    "# a tensor containing indices of tokens that you want to look up in the embedding layer\n",
    "input_indices = torch.LongTensor([1,5,3,2])\n",
    "\n",
    "# embedding lookup - get me the embeddings for the tokens at index 1,5,3,and 2 of the matrix\n",
    "embedded_output = embedding(input_indices)\n",
    "\n",
    "# the shape will be the number of indices, so 4 in this case, by the embedding_dim so 100 since each token is represented by 100 dimensions\n",
    "print(embedded_output.shape)\n",
    "\n",
    "# weight*x + bias = y\n",
    "# more specifically sigmoid(w_1*a_1+w_2*a_2+w_3*a_3+...+w_n*a_n + bias) \n",
    "# sigmoid is a funciton with a range of (0,1)\n",
    "# weight is a weight matrix, multiplied by the input data x (a value or vector)\n",
    "# bias is an additive hyperparameter helping the model fit data\n",
    "# the function of how data moves from one layer to another in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n",
      "tensor([[ 27,  30,  33],\n",
      "        [ 61,  68,  75],\n",
      "        [ 95, 106, 117]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "a = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "b = torch.tensor([[7,8,9],[10,11,12]])\n",
    "print(a@b)\n",
    "print(torch.matmul(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 0],\n",
      "        [1, 0],\n",
      "        [2, 3]])\n",
      "tensor([[0.5381, 0.1094, 0.2450],\n",
      "        [0.7957, 0.1353, 0.1590]])\n",
      "tensor([[2.7517, 0.7349],\n",
      "        [3.6361, 0.4771]])\n"
     ]
    }
   ],
   "source": [
    "int_64 = torch.randint(5,(3,2)) # returns a 3 by 2 matrix with random integer values going from 0 to 5\n",
    "print(int_64)\n",
    "float_32 = torch.rand(2,3) # returns a 2 by 3 matrix with random floating point numbers\n",
    "print(float_32)\n",
    "\n",
    "# you can cast integer to float to make matrix multiplication work\n",
    "print(torch.matmul(float_32,int_64.float()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
